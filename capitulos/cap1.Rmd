\chapter{Duties and Responsibilities}

# Purpose

Over the past decades, trade-led growth in Asia-Pacific has been credited with lifting hundreds of millions above the poverty line. There is substantial room in  the region to further reap benefits offered by international trade through regional integration and complementary trade policies. Exploiting this could support many developing countries in the region to get back on track to achieving the Sustainable Development Goals (SDGs).

To take full advantage of this potential, requires concerted efforts by member States to enhance regional trade cooperation and direct it towards coordination and integration. This dynamics is further increasingly complicated by the recent rise of protectionism, uncertainty with the future of the World Trade Organization (WTO) and global trade regime, rising significance of digital trade, Least Developed Countries (LDC) graduations, increasing complexity of the regional trade agreements and non-tariff trade policies. 

However, ESCAP member States, particularly LDCs, often lack the required analytical capacity to formulate evidence-based policies and assess their impacts, and are continuously making requests for technical assistance for developing such capacity.

The consultant work was conducted in support of the regular programme of technical cooperation project titled, "Strengthening analytical capacity of ESCAP members States to conduct analysis of impact of trade policy towards sustainable development" (project code RB23).

# Objective

The final goal of this consultancy project was to provided a set of open source tools that facilitates statistical modeling for better decision making in international trade and regional integration. This tool can be used by economists, lawyers, and different professionals who work for the RTAs (Regional Trade Agreements) negotiation teams or similar units in LDCs, so this was created with a specific user in mind, who tends to be more acquainted with Excel and SPSS than with a command line tools.

The need to use open source is that there is an obvious saving in software licenses, but it also eases reproducing studies, as most open source tools run on different platforms, scale easily and perform well with limited hardware resources. My solution can be run with zero direct monetary cost in a Windows/Mac laptop for reporting, a Linux/Unix server for large scale models or a combination of both.

Reproducibility is not just ethically desirable when tax payers money is being used to build models that support decisions that affect them, but it also fosters new research. Reproducibility is not just publishing your analysis code. The entire workflow of a research project --from formulating hypotheses to dissemination of your results-- has decisions and steps that ideally should be reproducible [@malexander19].

A side consequence of open source solutions is that these rely on open source, well established open formats, which can be read with zero compatibility problems with both open source software (e.g. Python) and closed source software (e.g. Stata or SPSS). Closed source formats are not fully retro compatible or cross plaform compatible, a good example is the impossibility of importing Stata files created with recent versions with older Stata editions, and the same happens with well extended propietary software such as Microsoft Excel. Open source formats such as the brand new Apache Arrow or classic fully specified formats such as CSV, XML or JSON lack the retro compatibility problem.

Any general open source programming language such as Python or Java could have worked. In this case, the most efficient decision was to stick to R, a computational statistics language that provides a set of functions, ideal for this goal, ready to be adapted and build on top. The deal breaker to decide in favor of R is that it features the `tidyverse` [@tidyverse], a set of functions built on top of R which facilitate statistical analysis and developing new statistical tools. In addition, R features a large and inclusive community organized around different organized groups such as R Users Group and R-Ladies who constantly organize open and free training activities.

# Ultimate result of service

The project started in September, 2020 and was completed in June, 2021. The *first stage* of the project consisted in fitting all the models from @yotov2016advanced by using R. This implementation, which required a combination of different existing tools, was organized into an R package, `tradepolicy` [@tradepolicy], with the goal of not just making the code organized, but also to ease reproducibility and providing users moving from Excel or Stata from R, which is known for being a flexible tool but with a steep learning curve.

All of this code effort, which involved, among other challenges, providing user-friendly functions to obtain clustered standard errors for regression summaries and methods to obtain a pseudo-$R^2$ in Generalized Linear Models (GLMs) that were not readily available in R, led to a *second stage* that consisted in providing extensive details to explain the data manipulation and model creation from the first stage. This was organized in the ebook @solutionsagtpa, which provides a comprehensive step-by-step guide to start from zero in the R programming language and guides the users towards enabling them at fitting their own general equilibrium models, which can be used to simulate, for example, the overall impact of a new tariff to imported goods, and the crossed effects that it introduces in the economy.

The *third stage* consisted in delivering live online training to ARTNeT members in December, 2020. This constituted a great opportunity to test both the ebook, which was greatly improved from participant's feedback. The developed R code was presented in support of parallel sessions of structural gravity training using STATA.

A test bank for the solutions manual was also created. Eighty multiple choice questions in digital format are available to test knowledge of ESCAP training participants. Only forty questions are used in the test, so many tests can be generated via loops with different randomization parameters.

The *fourth stage* of the project consisted in implementing clever fitting methods to avoid bottlenecks. GLMs in R (and also Python) tend to be 6 to 10 times slower than Stata when using Quasi-Poisson link, which is what is used in International Economy as a remedial method for overdisperssion. This constitutes a barrier for users who plan to adopt R and come from Stata, and this is also a problem for building interactive dashboards, which require immediateness.

Fitting general equilibrium models that are, by definition, slow to fit, is not a trivial task. This motivated the creation of `eflm`, which stands for Efficient Fitting of Linear Models, an R package that re-implements base R functions for LMs and GLMs. This effort, which required linking R, C and FORTRAN code, implements a weak formulation that reduces the model design matrix from $N\times P$ into $P\times P$ and therefore offers large time improvements that match Stata times.

A *fifth stage*, not initially included, was to build an interactive dashboard to simulate decision making by applying all the previous stages of this project. COMPLETAR.

@solutionsagtpa had two releases during this project. A first version delivered in December, 2020 and a second version delivered in June, 2021 with corrections and the inclusion of `eflm` to make R a competitive tool for economic analysis.
