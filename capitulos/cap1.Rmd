\chapter{Purpose, Objective and Deliverables}

# Purpose

Over the past decades, trade-led growth in Asia-Pacific has been credited with lifting hundreds of millions above the poverty line. There is substantial room in  the region to further reap benefits offered by international trade through regional integration and complementary trade policies. Exploiting this could support many developing countries in the region to get back on track to achieving the SDGs.

To take full advantage of this potential, requires concerted efforts by member States to enhance regional trade cooperation and direct it towards coordination and integration. This dynamics is further increasingly complicated by the recent rise of protectionism, uncertainty with the future of the WTO and global trade regime, rising significance of digital trade, LDC graduations, increasing complexity of the regional trade agreements and non-tariff trade policies. 

However, ESCAP member States, particularly LDCs, often lack the required analytical capacity to formulate evidence-based policies and assess their impacts, and are continuously making requests for technical assistance for developing such capacity. The consultant work was conducted in support of the regular programme of technical cooperation project titled, "Strengthening analytical capacity of ESCAP members States to conduct analysis of impact of trade policy towards sustainable development" (project code RB23).

# Objective

This work aims at obtaining a reproduction of @yotov2016advanced, a cornerstone book in gravity models literature, and provide an efficient and easy to use open source tool for estimating gravity models. Both results can be used a a basis to simulate, for example, the overall impact of a new tariff to imported goods, and the crossed effects that it introduces in the economy. Improving computational and usage aspects of gravity models facilitates statistical modeling for better decision making in international trade and regional integration, besides other areas of applications which are not the focus of this work.

Scientific analysis involves decisions and steps that ideally should be reproducible [@malexander19]. Reproducibility is ethically desirable when tax payers money is being used to build models that support decisions that affect them, it also fosters new research, and sheds light from formulating hypotheses to dissemination of scientific results. 

The results and tools created in this work are accessible for all professionals who work for the RTAs negotiation teams or similar units in LDCs, and who are not necessarily expert programmers.

The need to use open source is that there is an obvious saving in software licenses, but it also eases reproducing studies and scales well. Most open source tools run on different platforms, scale easily and perform well with limited hardware resources. My solution just works with zero direct monetary cost in a Windows/Mac laptop for reporting, a Linux/Unix server for large models or simulations, or a combination of both.

Any general open source programming language such as Python or Java could have served this purpose. R was purposely designed to make data analysis and statistics easier, not to be fast [@wickham2019advanced]. For the goal of reproducing results, R is perfect as it includes many statistical functions to build on top. While R is slow compared to other programming languages, for most purposes, itâ€™s fast enough, but not at GLMs, which can constitute an adoption barrier for people work in finance and economics, as they tend to use Stata which is faster at this. Therefore, making R competitive at fitting GLMs is a technically desirable goal.

<!-- A side consequence of open source solutions is that these rely on well established open formats, which can be read with zero compatibility problems with both open source software (e.g. Python) and closed source software (e.g. Stata or SPSS). Closed source formats are not fully retro compatible or cross platform compatible, a good example is the impossibility of importing Stata files created with recent versions by using older Stata editions, and the same happens with well extended proprietary software such as Microsoft Excel. Open source formats such as the brand new Apache Arrow or classic fully specified formats such as CSV, XML or JSON lack the retro compatibility problem. -->

# Deliverables

## First stage

Implementing all the models from @yotov2016advanced by using R. This implementation, which required a combination of different existing tools, was organized into an R package, `tradepolicy` [@tradepolicy], with the goal of making the code organized with the new functions to obtain model metrics. This effort involved providing:

* User-friendly functions to obtain clustered standard errors for regression summaries, something not easily done with R.
* A pseudo-$R^2$ for Generalized Linear Models (GLMs) that didn't exist in R, which is based on @silva2006log.

## Second stage

Documenting the results in the ebook @solutionsagtpa, which provides a comprehensive step-by-step guide to start from zero in the R programming language and guides the users towards enabling them at fitting their own general equilibrium models.

At this point all the fitted models could be reproduced, including figures and summaries from @yotov2016advanced. The solely differences were model intercepts because of how Stata and R differ at some steps of GLMs fitting, but the slopes, standard errors and every model assessment metric could be fully replicated.

The replication work was somewhat straightforward because of the availability of:

* Code and datasets, both documented.
* Comprehensive explanations from theory, which allowed to provide a meaningful translation from Stata to R, even when the translation was not 1:1 because of difference on how both languages deal with categorical variables.
* Direct communication with the principal author, who replied all of my questions about the data cleaning steps.

## Third stage

Delivering live online training to ARTNeT members in December, 2020. This constituted a great opportunity to test both the ebook, which was greatly improved from participant's feedback. The developed R code was presented in support of parallel sessions of structural gravity training using STATA, and helped to improve the explanations contained in the ebook. @solutionsagtpa had two releases during this project. A first version delivered in December, 2020 and a second version delivered in June, 2021 with corrections and the inclusion of `eflm` to make R a competitive tool for economic analysis.

A test bank for the solutions manual was also created as a part of this stage. Eighty multiple choice questions in digital format are available to test knowledge of ESCAP training participants. Only forty questions are used in the test, so many tests can be generated via loops with different randomization parameters.

## Fourth stage

Implementing clever fitting methods to avoid bottlenecks. GLMs in R (and also Python) tend to be at least 3 times slower than Stata depending on the employed link function and the number of categorial variables. For the case of gravity models, it was of central interest to optimize the fitting process for the Quasi-Poisson family, which is what is used in International Economy as a remedial method for over-disperssion. The need of speed is about being able to build interactive dashboards to simulate the impact of policy decisions.

Fitting general equilibrium models that are, by definition, slow to fit, is not a trivial task. This motivated the creation of `eflm`, which stands for Efficient Fitting of Linear Models, an R package that re-implements base R functions for LMs and GLMs. This effort, which required linking R, C and FORTRAN code, implements a weak formulation that reduces the model design matrix from $N\times P$ into $P\times P$ and therefore offers large time improvements that match Stata times.

## Fifth stage

EL DASHBOARD
