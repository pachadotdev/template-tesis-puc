\documentclass[12pt,reqno,oneside,pdftex]{formato-puc/puctesis} % For pdflatex
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algorithm2e}
\usepackage{fancybox}
\usepackage{float}
\usepackage{times}
\usepackage{inconsolata}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
  
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
%\usepackage[latin1]{inputenc}
                                
\addto\captionsspanish{
  \renewcommand{\contentsname}{INDICE GENERAL}
}
\addto\captionsspanish{
 \renewcommand{\listfigurename}{INDICE DE FIGURAS}
}
\addto\captionsspanish{
 \renewcommand{\listtablename}{INDICE DE TABLAS}
}
\addto\captionsspanish{
 \renewcommand{\chaptername}{CAPITULO}
}
\addto\captionsspanish{
 \renewcommand{\figurename}{Figura}
}
\addto\captionsspanish{
 \renewcommand{\tablename}{Tabla}
}

\newtheorem{definicion}{\bf Definici\'on}[chapter]
\newtheorem{propiedad}{Propiedad}[chapter]
\newtheorem{afirmacion}{Afirmaci\'on}[chapter]
\newtheorem{lema}{\bf Lema}[chapter]
\newtheorem{proposicion}{Proposici\'on}[chapter]
\newtheorem{teorema}{\noindent \bf Teorema}[chapter]
\newtheorem{corolario}{\bf Corolario}[chapter]
\newtheorem{pf}{Demostraci\'on}[chapter]
\newtheorem{ejemplo}{\bf Ejemplo}[chapter]
\newtheorem{comentario}{Comentario}[chapter]

\newcommand\opgrad{\operatorname{grad}}    

% para texlive 2020
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}
  {}
  {\par}

\begin{document}

\title{Gravity Modelling Into R: Strengthening Analytical Capacity of
ESCAP Members States}
\author{Mauricio `Pachá' Vargas Sepúlveda}

\degree       {Master of Science in Statistics} 
\advisor      {Constanza Prado Stuardo}
\committeememberA      {Alexey Kravchenko (United Nations)}
\committeememberB      {Yoto V Yotov (Drexel University)}
\date         {June 2021}
\copyrightname{Mauricio `Pachá' Vargas Sepúlveda}
\copyrightyear{MMXXI}
\dedication   {To my family, friends, and all the valuable people from
PUC.}

\PageNumbersFootCentered
\pagenumbering{roman}
\maketitle

\chapter*{AGRADECIMIENTOS}
Esta plantilla de R Markdown se base en la plantilla Latex hecha por
Miguel Torres Torriti. Los agradecimientos se editan directamente en formato-puc/base.tex.
\par

\cleardoublepage
\tableofcontents
\listoffigures          
\listoftables           
\cleardoublepage

\chapter*{DISCLAIMER}

The views and opinions expressed in this thesis project are solely those of the author and do not necessarily reflect the official position of any unit of the World Trade Organization, the United Nations Conference on Trade and Development, the United States International Trade Commission nor the Pontifical Catholic University of Chile.

\cleardoublepage % In double-sided printing style makes the next page 
                 % a right-hand page, (i.e. a truly odd-numbered page 
                 % with respect to absolut counting), producing a blank
                 % page if necessary. Added by MTT 20.AUG.2002 

\NoChapterPageNumber           % elimina encabezado - pie de pagina de la
                               % primera pagina de cada capitulo
\pagenumbering{arabic}

\chapter{Duties and Responsibilities}

\hypertarget{purpose}{%
\section{Purpose}\label{purpose}}

Over the past decades, trade-led growth in Asia-Pacific has been
credited with lifting hundreds of millions above the poverty line. There
is substantial room in the region to further reap benefits offered by
international trade through regional integration and complementary trade
policies. Exploiting this could support many developing countries in the
region to get back on track to achieving the Sustainable Development
Goals (SDGs).

To take full advantage of this potential, requires concerted efforts by
member States to enhance regional trade cooperation and direct it
towards coordination and integration. This dynamics is further
increasingly complicated by the recent rise of protectionism,
uncertainty with the future of the World Trade Organization (WTO) and
global trade regime, rising significance of digital trade, Least
Developed Countries (LDC) graduations, increasing complexity of the
regional trade agreements and non-tariff trade policies.

However, ESCAP member States, particularly LDCs, often lack the required
analytical capacity to formulate evidence-based policies and assess
their impacts, and are continuously making requests for technical
assistance for developing such capacity.

The consultant work was conducted in support of the regular programme of
technical cooperation project titled, ``Strengthening analytical
capacity of ESCAP members States to conduct analysis of impact of trade
policy towards sustainable development'' (project code RB23).

\hypertarget{objective}{%
\section{Objective}\label{objective}}

The final goal of this consultancy project was to provided a set of open
source tools that facilitates statistical modeling for better decision
making in international trade and regional integration. This tool can be
used by economists, lawyers, and different professionals who work for
the RTAs (Regional Trade Agreements) negotiation teams or similar units
in LDCs, so this was created with a specific user in mind, who tends to
be more acquainted with Excel and SPSS than with a command line tools.

The need to use open source is that there is an obvious saving in
software licenses, but it also eases reproducing studies, as most open
source tools run on different platforms, scale easily and perform well
with limited hardware resources. My solution can be run with zero direct
monetary cost in a Windows/Mac laptop for reporting, a Linux/Unix server
for large scale models or a combination of both.

Reproducibility is not just ethically desirable when tax payers money is
being used to build models that support decisions that affect them, but
it also fosters new research. Reproducibility is not just publishing
your analysis code. The entire workflow of a research project --from
formulating hypotheses to dissemination of your results-- has decisions
and steps that ideally should be reproducible (Alexander 2019).

A side consequence of open source solutions is that these rely on open
source, well established open formats, which can be read with zero
compatibility problems with both open source software (e.g.~Python) and
closed source software (e.g.~Stata or SPSS). Closed source formats are
not fully retro compatible or cross plaform compatible, a good example
is the impossibility of importing Stata files created with recent
versions with older Stata editions, and the same happens with well
extended propietary software such as Microsoft Excel. Open source
formats such as the brand new Apache Arrow or classic fully specified
formats such as CSV, XML or JSON lack the retro compatibility problem.

Any general open source programming language such as Python or Java
could have worked. In this case, the most efficient decision was to
stick to R, a computational statistics language that provides a set of
functions, ideal for this goal, ready to be adapted and build on top.
The deal breaker to decide in favor of R is that it features the
\texttt{tidyverse} (Wickham et al. 2019), a set of functions built on
top of R which facilitate statistical analysis and developing new
statistical tools. In addition, R features a large and inclusive
community organized around different organized groups such as R Users
Group and R-Ladies who constantly organize open and free training
activities.

\hypertarget{ultimate-result-of-service}{%
\section{Ultimate result of service}\label{ultimate-result-of-service}}

The project started in September, 2020 and was completed in June, 2021.
The \emph{first stage} of the project consisted in fitting all the
models from Yotov et al. (2016) by using R. This implementation, which
required a combination of different existing tools, was organized into
an R package, \texttt{tradepolicy} (Vargas 2021b), with the goal of not
just making the code organized, but also to ease reproducibility and
providing users moving from Excel or Stata from R, which is known for
being a flexible tool but with a steep learning curve.

All of this code effort, which involved, among other challenges,
providing user-friendly functions to obtain clustered standard errors
for regression summaries and methods to obtain a pseudo-\(R^2\) in
Generalized Linear Models (GLMs) that were not readily available in R,
led to a \emph{second stage} that consisted in providing extensive
details to explain the data manipulation and model creation from the
first stage. This was organized in the ebook Vargas (2021a), which
provides a comprehensive step-by-step guide to start from zero in the R
programming language and guides the users towards enabling them at
fitting their own general equilibrium models, which can be used to
simulate, for example, the overall impact of a new tariff to imported
goods, and the crossed effects that it introduces in the economy.

The \emph{third stage} consisted in delivering live online training to
ARTNeT members in December, 2020. This constituted a great opportunity
to test both the ebook, which was greatly improved from participant's
feedback. The developed R code was presented in support of parallel
sessions of structural gravity training using STATA.

A test bank for the solutions manual was also created. Eighty multiple
choice questions in digital format are available to test knowledge of
ESCAP training participants. Only forty questions are used in the test,
so many tests can be generated via loops with different randomization
parameters.

The \emph{fourth stage} of the project consisted in implementing clever
fitting methods to avoid bottlenecks. GLMs in R (and also Python) tend
to be 6 to 10 times slower than Stata when using Quasi-Poisson link,
which is what is used in International Economy as a remedial method for
overdisperssion. This constitutes a barrier for users who plan to adopt
R and come from Stata, and this is also a problem for building
interactive dashboards, which require immediateness.

Fitting general equilibrium models that are, by definition, slow to fit,
is not a trivial task. This motivated the creation of \texttt{eflm},
which stands for Efficient Fitting of Linear Models, an R package that
re-implements base R functions for LMs and GLMs. This effort, which
required linking R, C and FORTRAN code, implements a weak formulation
that reduces the model design matrix from \(N\times P\) into
\(P\times P\) and therefore offers large time improvements that match
Stata times.

A \emph{fifth stage}, not initially included, was to build an
interactive dashboard to simulate decision making by applying all the
previous stages of this project. COMPLETAR.

Vargas (2021a) had two releases during this project. A first version
delivered in December, 2020 and a second version delivered in June, 2021
with corrections and the inclusion of \texttt{eflm} to make R a
competitive tool for economic analysis.

\chapter{Replication of `An Advanced Guide To Trade Policy Analysis'}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

Despite solid theory and remarkable empirical success, the gravity
equation is often applied a-theoretically and without account for
estimation challenges that may render estimates biased/inconsistent.

Much of gravity models found in the literature fall under this quote
from Leonardo da Vinci: \emph{``He who loves practice without theory is
like the sailor who boards ship without a rudder and compass and never
knows where he may cast.''}

Traditionally, trade theory and trade policy analysis are performed in a
general equilibrium setting. However, doing competent general
equilibrium analysis is not a trivial task.

Modelers must know general-equilibrium theory so that their models have
a sound theoretical basis; they must know how to solve their models;
they need to be able to program (or at least communicate with
programmers); they have to know about data sources and all their
associated problems; and they have to be conversant with relevant
literature, especially that on elasticities (Shoven and Whalley 1984).

The gravity model has established itself as the `workhorse' model in
international trade, and thousands of papers have used it to study the
effects of various determinants of trade.

Traditional determinants of trade flows are: Geography, Preferential
Trade Agreements, Tariffs, Export Subsidies, Non-tariff Measures, World
Trade Organization Membership, Common Currency and Currency Unions,
Foreign Direct Investment, Immigration, Cultural Ties, Language,
Colonial Ties.

Exotic determinants of trade flows are: Institutional Quality, Foreign
Aid, Trust, Reputation for People, Reputation for Products, Export
Promotion, Taxes, Mega Sporting Events (Olympic Games and World Cup),
Embargoes, Other Trade Sanctions, Conflict, Wars, Piracy, and Ice Cap
Melting.

The gravity model can simultaneously accommodate multiple countries and
multiple sectors (and even firms). As such, the gravity framework can be
used to capture the possibility that markets (sectors, countries, etc.)
are linked and that trade policy changes in one market will trigger
ripple effects in the rest of the world.

Structural Gravity translates the effects of the universe of bilateral
trade policies to the country level and decomposes their incidence on
producers and consumers in the world. Thus, the trade gravity model can
be conveniently integrated within a wide class of broader general
equilibrium models to study the links between trade, labor markets,
investment, the environment, etc., while preserving tractability.

\hypertarget{gravity-models-review}{%
\section{Gravity models review}\label{gravity-models-review}}

\hypertarget{simple-gravity-model}{%
\subsection{Simple gravity model}\label{simple-gravity-model}}

The main reference for this section is Woelwer et al. (2018) and the
references therein. Gravity models in their traditional form are
inspired by Newton law of gravitation \begin{equation*}
F_{ij}=G\frac{M_{i}M_{j}}{D^{2}_{ij}}.
\end{equation*}

The force \(F\) between two bodies \(i\) and \(j\) with \(i \neq j\) is
proportional to the masses \(M\) of these bodies and inversely
proportional to the square of their geographical distance \(D\). \(G\)
is a constant and as such of no major concern. The underlying idea of a
traditional gravity model, shown for international trade, is equally
simple \begin{equation*}
X_{ij}=G\frac{Y_{i}^{\beta_{1}}Y_{j}^{\beta_{2}}}{D_{ij}^{\beta_{3}}}.
\end{equation*}

The trade flow \(X\) is explained by \(Y_{i}\) and \(Y_{j}\) that are
the masses of the exporting and importing country (e.g.~the GDP) and
\(D_{ij}\) that is the distance between the countries. This is also used
to study urban policies and migration flows.

Dummy variables such as common borders \(contig\) or regional trade
agreements \(rta\) can be added to the model. Let \(t_{ij}\) be the
transaction cost defined as
\(t_{ij}= D_{ij} \exp(CONTIG_{ij} + RTA_{ij})\), therefore the model
with frictions can be expressed as a log-linear model to use a standard
estimation methods such as OLS \begin{align*}
X_{ij} =& G\frac{Y_{i}^{\beta_{1}}Y_{j}^{\beta_{2}}}{t_{ij}^{\beta_{3}}} \\
\implies \log X_{ij} =& \beta_{0}\log G +\beta_{1}\log Y_{i}+\beta_{2}\log Y_{j}+\beta_{3}\log D_{ij}\\
&+ \beta_{4}CONTIG_{ij}+\beta_{5}RTA_{ij}.
\end{align*}

\hypertarget{trade-barriers-model}{%
\subsection{Trade barriers model}\label{trade-barriers-model}}

Basically the model proposes that the exports \(X_{ij}\) from \(i\) to
\(j\) are determined by the supply factors in \(i\), \(Y_{i}\), and the
demand factors in \(j\), \(Y_{j}\), as well as the transaction costs
\(t_{ij}\).

Next to information on bilateral partners \(i\) and \(j\), information
on the rest of the world is included in the gravity model in the
equation \(Y=\sum_{i} Y_{i}= \sum_{j} Y_{j}\) that represents the
worldwide sum of incomes (e.g.~the world's GDP).

A key assumption is to take a fixed value \(\sigma > 1\) in order to
account for the preference for a variation of goods (e.g.~in this model
goods can be replaced for other similar goods).

The Multilateral Resistance terms are included via the terms \(P\),
Inward Multilateral Resistance, and \(\Pi\), Outward Multilateral
Resistance. The Inward Multilateral Resistance \(P_i\) is a function of
the transaction costs of \(i\) to all trade partners \(j\). The Outward
Multilateral Resistance \(\Pi_{j}\) is a function of the transaction
costs of \(j\) to all trade partners \(i\) and their demand. The
Multilateral Resistance terms dependent on each other. Hence, the
estimation of structural gravity models becomes complex.

The econometric literature proposes the Multilateral Resistance model
defined by the equations \begin{align*}
X_{ij} =& \frac{Y_{i}Y_{j}}{Y}\frac{t_{ij}^{1-\sigma}}{P_{j}^{1-\sigma}\Pi_{i}^{1-\sigma}}\\
P_{i}^{1-\sigma} \text{ }& \sum_{j}\frac{t_{ij}^{1-\sigma}}{\Pi_{j}^{1-\sigma}}\frac{Y_{j}}{Y};\:\Pi_{j}^{1-\sigma}=\sum_{i}\frac{t_{ij}^{1-\sigma}}{P_{i}^{1-\sigma}}\frac{Y_{i}}{Y}
\end{align*}

\hypertarget{model-estimation}{%
\subsection{Model estimation}\label{model-estimation}}

To estimate gravity equations you need a square dataset including
bilateral flows defined by the argument \texttt{dependent\_variable}, a
distance measure defined by the argument \texttt{distance} that is the
key regressor, and other potential influences (e.g.~contiguity and
common currency) given as a vector in \texttt{additional\_regressors}
are required.

Some estimation methods require ISO codes or similar of type character
variables to compute particular country effects, and the rule of thumb
for independent variables consists in that all dummy variables should be
of type numeric (0/1) and f an independent variable is defined as a
ratio, it should be logged.

\hypertarget{models-replication}{%
\section{Models replication}\label{models-replication}}

\hypertarget{work-organization}{%
\subsection{Work organization}\label{work-organization}}

The R package \texttt{tradepolicy} (Vargas 2021b) allows easy summaries
for partial and general equilibrium models. The detailed model
replication is available from my ebook
\href{https://r.tiid.org/R_structural_gravity/}{Solutions Manual for An
Advanced Guide to Trade Policy Analysis} published by UN ESCAP, and it's
a material intended to provide a comprehensive explanation to reproduce
the results from Yotov et al. (2016) in R. This ebook is not an attempt
to give a thorough discussion of the theory behind gravity models, the
references cited through the chapters shall fill those details.

From the considerations from Chapter 1, I had two audiences in mind:
People who know R and are interested in learning about gravity models
and people with no R knowledge who know gravity models theory. This
assumes that the readers are familiar with linear regression, and that
they shall read Yotov et al. (2016) and Wickham and Grolemund (2016)
alongside this material.

\hypertarget{previous-and-side-work}{%
\subsection{Previous and side work}\label{previous-and-side-work}}

A previous work, \texttt{gravity} (Woelwer et al. 2018), intented for
general gravity models estimation in R, provides a wrapper of different
standard estimation methods for gravity models. This facilitates
estimation of both log-log and multiplicative models.

In addition, \texttt{cepiigeodist} (Vargas 2020) eases gravity modelling
in R as it provides data on countries and their main city or
agglomeration and the different distance measures and dummy variables
indicating whether two countries are contiguous, share a common language
or a colonial relationship. The reference article for these datasets is
Mayer and Zignago (2011).

\hypertarget{partial-equilibrium-estimation}{%
\subsection{Partial equilibrium
estimation}\label{partial-equilibrium-estimation}}

\hypertarget{traditional-gravity-estimates}{%
\subsubsection{Traditional gravity
estimates}\label{traditional-gravity-estimates}}

The most simple model considered in \texttt{tradepolicy} is an OLS
estimation ignoring multilateral resistance terms \begin{align*}
\log X_{ij,t} =& \beta_0 + \beta_1 DIST_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 LANG_{i,j}\\ 
\text{ }& + \beta_4 CLNY_{i,j} + \beta_5 \log Y_{i,t} + \beta_6 \log E_{j,t} + \varepsilon_{ij,t}.
\end{align*}

The model can be summarised in the exact way as reported in Yotov et al.
(2016). The first challenge was to obtain country pair clustered
standard errors for the estimated coefficients (\(\hat{\beta}\)), to
provide an \(R^2\) or pseudo-\(R^2\) metric, alongside Ramsey Regression
Equation Specification Error Test (RESET) test p-value. This mimics
Stata regression output which is different from R results presentation.
All of this is readily available in the \texttt{tp\_summary\_app1()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app1}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"log\_trade \textasciitilde{} log\_dist + cntg + lang + clny + log\_y + log\_e"}\NormalTok{,}
  \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(ch1\_application1, trade }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{),}
  \AttributeTok{method =} \StringTok{"lm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 7 x 5
  term        estimate std.error statistic   p.value
  <chr>          <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept)  -11.3     0.296      -38.1  1.17e-309
2 log_dist      -1.00    0.0273     -36.6  1.85e-286
3 cntg           0.574   0.185        3.11 1.90e-  3
4 lang           0.802   0.0821       9.76 1.78e- 22
5 clny           0.735   0.144        5.10 3.49e-  7
6 log_y          1.19    0.00946    126.   0        
7 log_e          0.908   0.00991     91.6  0        

$nobs
[1] 25689

$rsquared
[1] 0.7585251

$etfe
[1] FALSE

$itfe
[1] FALSE

$reset_pval
[1] 4.346285e-15
\end{verbatim}

In this output \texttt{etfe}/\texttt{itfe} refers to the presence of
Exporter/Importer time fixed effects in the model and \texttt{nobs} is
the number of rows used for the estimation after discarding unusable
cases.

The package documentation covers all the details on each of the function
arguments and the output, but here it is important to mention that
clustered standard errors are not econometricians's sophistication.
Estimations are clustered by trading pair in order to account for any
intra-cluster correlations at the trading pair level. While important
bilateral time-varying effects are controlled for with explanatory
variables, such as RTAs, and any bilateral time-invariant effects are
taken into account with fixed effects, some correlation pattern between
pairs of countries over time may still be present in the error term.
This correlation pattern is captured by clustering the errors over
country-pairs Yotov et al. (2016).

OLS estimation controlling with remote indexes and fixed effects are
quite similar to this model, both don't pass the RESET test as the
simple OLS model just shown. It is Pseudo-Poisson Maximum Likelihood
(PPML) estimation the one that is interesting both statistically and
economically. PPML proposes a multiplicative model that passes RESET
test, and it requires to be estimated by using a Quasi-Poisson GLM.

PPML gravity model is defined as \begin{align*}
X_{ij,t} =& \exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} +\right.\\
\text{ }& \left.\beta_3 LANG_{i,j} + \beta_4 CLNY_{i,j}\right] \times \varepsilon_{ij,t}.
\end{align*}

Where the added terms, with respect to the OLS model, are \(\pi_{i,t}\)
and \(\chi_{i,t}\) that account for exporter-time and importer-time
fixed effects respectively.

The reason to compute this model even in spite of speed is that PPML is
the only estimator that is perfectly consistent with the theoretical
gravity model. By estimating with PPML the fixed effects correspond
exactly to the corresponding theoretical terms.

And starting with this model, \texttt{tradepolicy} starts to show some
advantages over fitting the models from scratch by using the
\texttt{glm()} function. The pseudo-\(R^2\) for this model needs to be
computed after estimating the model because there is no \(R^2\) for
these models in R. The pseudo-\(R^2\) is obtained as a function of the
Kendall correlation between the observed and predicted values (Silva and
Tenreyro 2006).

The summary output, with hidden fixed effects, is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app1}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"trade \textasciitilde{} log\_dist + cntg + lang + clny + exp\_year + imp\_year"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ ch1\_application1,}
  \AttributeTok{method =} \StringTok{"glm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 5 x 5
  term        estimate std.error statistic   p.value
  <chr>          <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept)   10.4      0.452      23.0  2.29e-117
2 log_dist      -0.841    0.0317    -26.6  1.80e-155
3 cntg           0.437    0.0832      5.26 1.44e-  7
4 lang           0.247    0.0765      3.23 1.22e-  3
5 clny          -0.222    0.116      -1.91 5.56e-  2

$nobs
[1] 28152

$rsquared
[1] 0.5859927

$etfe
[1] TRUE

$itfe
[1] TRUE

$reset_pval
[1] 0.6416118
\end{verbatim}

\hypertarget{the-distance-puzzle}{%
\subsubsection{The distance puzzle}\label{the-distance-puzzle}}

The distance puzzle models accounts for a separation of the
\texttt{log\_dist} variable into multiple columns that account for
discrete time effects. This is the model specification \begin{align*}
X_{ij,t} =& \exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 LANG_{i,j}\right]\times\\
\text{ }& \exp\left[\beta_4 CLNY_{i,j} + \beta_5 \log(DIST\_INTRA_{i,i})\right] \times \varepsilon_{ij,t}.
\end{align*}

The distance puzzle can be estimated via OLS or PPML. For OLS you need
to exclude zero flows and loops in country pairs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app2}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"log\_trade \textasciitilde{} 0 + log\_dist\_1986 + log\_dist\_1990 + log\_dist\_1994 +}
\StringTok{    log\_dist\_1998 + log\_dist\_2002 + log\_dist\_2006 + cntg +}
\StringTok{    lang + clny + exp\_year + imp\_year"}\NormalTok{,}
  \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(ch1\_application2, importer }\SpecialCharTok{!=}\NormalTok{ exporter, trade }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{),}
  \AttributeTok{method =} \StringTok{"lm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 9 x 5
  term          estimate std.error statistic   p.value
  <chr>            <dbl>     <dbl>     <dbl>     <dbl>
1 log_dist_1986   -1.17     0.0436    -26.8  9.33e-156
2 log_dist_1990   -1.16     0.0423    -27.3  1.10e-161
3 log_dist_1994   -1.21     0.0457    -26.5  1.07e-152
4 log_dist_1998   -1.25     0.0428    -29.2  4.14e-184
5 log_dist_2002   -1.24     0.0441    -28.1  1.34e-171
6 log_dist_2006   -1.26     0.0437    -28.9  4.02e-180
7 cntg             0.223    0.203       1.10 2.71e-  1
8 lang             0.661    0.0821      8.06 8.20e- 16
9 clny             0.670    0.149       4.49 7.25e-  6

$nobs
[1] 25689

$pct_chg_log_dist
[1] 7.950156

$pcld_std_err
[1] 3.75886

$pcld_std_err_pval
[1] 0.03442616

$intr
[1] FALSE

$csfe
[1] FALSE
\end{verbatim}

A useful metric for the decomposition of \texttt{log\_dist} here is
\texttt{pct\_chg\_log\_dist}, which is the percentage change of the log
distance coefficients as a time-varying effect. This is obtained by
implementing the delta method on the distance coefficients weighted by
the country pair clustered variance-covariance matrix.

Por PPML you only need to exclude loops, but keeping zerop flows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app2}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"trade \textasciitilde{} 0 + log\_dist\_1986 + log\_dist\_1990 +}
\StringTok{    log\_dist\_1994 + log\_dist\_1998 + log\_dist\_2002 + log\_dist\_2006 +}
\StringTok{    cntg + lang + clny + exp\_year + imp\_year"}\NormalTok{,}
  \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(ch1\_application2, importer }\SpecialCharTok{!=}\NormalTok{ exporter),}
  \AttributeTok{method =} \StringTok{"glm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 9 x 5
  term          estimate std.error statistic   p.value
  <chr>            <dbl>     <dbl>     <dbl>     <dbl>
1 log_dist_1986   -0.859    0.0370    -23.2  5.09e-119
2 log_dist_1990   -0.834    0.0377    -22.1  1.42e-108
3 log_dist_1994   -0.835    0.0351    -23.8  4.09e-125
4 log_dist_1998   -0.847    0.0354    -23.9  9.44e-127
5 log_dist_2002   -0.848    0.0316    -26.8  2.66e-158
6 log_dist_2006   -0.836    0.0312    -26.7  1.53e-157
7 cntg             0.437    0.0832      5.26 1.46e-  7
8 lang             0.248    0.0765      3.23 1.22e-  3
9 clny            -0.222    0.116      -1.91 5.60e-  2

$nobs
[1] 28152

$pct_chg_log_dist
[1] -2.750345

$pcld_std_err
[1] 3.003899

$pcld_std_err_pval
[1] 0.3598811

$intr
[1] FALSE

$csfe
[1] FALSE
\end{verbatim}

Other model variations, such as internal distance solution for the
``distance puzzle,'' internal distance and home bias solution for the
``distance puzzle'' and the fixed effects solution for the ``distance
puzzle'' are very similar to this approach.

\hypertarget{regional-trade-agreements-effects}{%
\subsubsection{Regional trade agreements
effects}\label{regional-trade-agreements-effects}}

This model specification includes gravity covariates, including both
importer and exporter time fixed effects, and is a refinement over
traditional gravity estimates model \begin{align*}
X_{ij,t} =& \:\exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 LANG_{i,j} +\right.\\
\text{ }& \:\left.\beta_4 CLNY_{i,j} + \beta_5 RTA_{ij,t}\right] \times \varepsilon_{ij,t}.
\end{align*}

The distance puzzle can be estimated via OLS or PPML, but based on the
similarity with the previous models, you can go right into PPML model
fitting starting with the most simplified case

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app3}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"trade \textasciitilde{} 0 + log\_dist + cntg + lang + clny +}
\StringTok{    rta + exp\_year + imp\_year"}\NormalTok{,}
  \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(ch1\_application3, importer }\SpecialCharTok{!=}\NormalTok{ exporter),}
  \AttributeTok{method =} \StringTok{"glm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 5 x 5
  term     estimate std.error statistic   p.value
  <chr>       <dbl>     <dbl>     <dbl>     <dbl>
1 log_dist   -0.822    0.0310    -26.5  5.87e-155
2 cntg        0.416    0.0828      5.02 5.20e-  7
3 lang        0.250    0.0767      3.26 1.12e-  3
4 clny       -0.205    0.114      -1.80 7.23e-  2
5 rta         0.191    0.0658      2.90 3.75e-  3

$nobs
[1] 28152

$total_rta_effect
[1] 0.1907176

$trta_std_err
[1] 0.06579676

$trta_std_err_pval
[1] 0.003748494

$intr
[1] FALSE
\end{verbatim}

In this model summary we can see the same output as the previous model,
with the addition of \texttt{total\_rta\_effect} which is just the sum
of RTAs estimated coefficients and standard erro obtained with the Delta
method. This model does not feature intra-national distance effects
(\texttt{intr}).

Addressing for potential domestic trade diversion requires the inclusion
of an international borders (\texttt{intl\_brdr}) variable, a
factor-type variable which starts to make the estimation slow and from
here model estimation is computationally costly because of the number of
estimated parameters, but this does not include general equilibrium
effects yet.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tp\_summary\_app3}\NormalTok{(}
  \AttributeTok{formula =} \StringTok{"trade \textasciitilde{} 0 + log\_dist + cntg + lang + clny +}
\StringTok{    rta + exp\_year + imp\_year + intl\_brdr"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ ch1\_application3,}
  \AttributeTok{method =} \StringTok{"glm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$tidy_coefficients
# A tibble: 5 x 5
  term     estimate std.error statistic   p.value
  <chr>       <dbl>     <dbl>     <dbl>     <dbl>
1 log_dist   -0.800    0.0303    -26.4  4.21e-154
2 cntg        0.393    0.0789      4.99 6.19e-  7
3 lang        0.244    0.0771      3.16 1.57e-  3
4 clny       -0.182    0.113      -1.61 1.08e-  1
5 rta         0.409    0.0688      5.94 2.93e-  9

$nobs
[1] 28566

$total_rta_effect
[1] 0.4085219

$trta_std_err
[1] 0.06882661

$trta_std_err_pval
[1] 2.929108e-09

$intr
[1] TRUE
\end{verbatim}

These models constitute the basis for testing for potential ``reverse
causality'' between trade and RTAs and addressing potential non-linear
and phasing-in effects of RTAs.

\hypertarget{general-equilibrium-estimation}{%
\subsection{General equilibrium
estimation}\label{general-equilibrium-estimation}}

\hypertarget{core-ces-model}{%
\subsubsection{Core CES model}\label{core-ces-model}}

The gravity model can simultaneously accommodate multiple countries and
multiple sectors (and even firms). As such, the gravity framework can be
used to capture the possibility that markets (sectors, countries, etc.)
are linked and that trade policy changes in one market will trigger
ripple effects in the rest of the world.

Structural Gravity translates the effects of the universe of bilateral
trade policies to the country level and decomposes their incidence on
producers and consumers in the world.

Some may argue that the great predictive power of the empirical gravity
model is due to the use of a rich set of fixed effects
\(N \times T \times 2\), where \(N\) denotes countries and \(T\) denotes
years. It's possible to replace the \(N \times T \times 2\) dummies by
two remoteness indexes to produce exactly the same results and fit Baier
and Bergstrand (2009). Thus, the predictive power of the empirical
gravity model is indeed unprecedented.

The foundational stone of the structural gravity model is Anderson
(1979), which assumes that product differentiation exists by place of
origin, that consumers globally have preferences that are truthfully
represented by a Constant Elasticity of Substitution (CES) utility
function, and that the equilibrium condition is a budget equilibrium. In
this model, budgeting is defined in two stages, the lower level
determines bilateral allocation and the upper level determines aggregate
output and expenditures in each country.

Under Anderson's fundamental model, consumer's maximize their utility
under budget constraints, which means to solve the next optimization
problem \begin{equation*}
\max \left\{ \sum_{i\in I} \beta_i^{\frac{1 - \sigma}{\sigma}} c_{ij}^{\frac{\sigma - 1}{\sigma}} \right\}^{\frac{\sigma - 1}{\sigma}} \quad \text{s.t.} \quad \sum_{i \in I} p_{ij} c_{ij} = E_j = Y_j.
\end{equation*}

Where

\begin{itemize}
\tightlist
\item
  \(c_{ij}\) is consumption in destination \(j\) of goods from source
  \(i\)
\item
  \(\sigma > 1\) is the elasticity of substitution across
  varieties/sources
\item
  \(\beta_i\) is a CES share parameter
\item
  \(p_{ij} = p_i t_{ij}\) is the price at destination \(j\)
\item
  \(p_i\) is the factory-gate/mill price at source \(i\)
\item
  \(t_ij > 1\) denotes trade frictions for shipping goods from \(i\) to
  \(j\)
\item
  \(E_j\) is the total expenditure at destination \(j\)
\item
  \(Y_j = p_j Q_j\) is the total income at \(j\), where \(Q_j\) is the
  endowment
\end{itemize}

The target function can be maximized via Karush-Kuhn-Tucker algorithm
with a restriction of the form \(k - g(x) \geq 0\) for multiplier
interpretation as shadow-price \begin{equation*}
L = \left\{ \sum_{i\in I} \beta_i^{\frac{1 - \sigma}{\sigma}} c_{ij}^{\frac{\sigma - 1}{\sigma}} \right\}^{\frac{\sigma - 1}{\sigma}} + \mu \left( Y_j - \sum_{i \in I} p_{ij} c_{ij} \right).
\end{equation*}

Applying chain rule to obtain First Order Conditions (FOCs) with respect
to \(c_{ij}\) \begin{equation*}
\left\{ \sum_{i\in I} \beta_i^{\frac{1 - \sigma}{\sigma} - 1} c_{ij}^{\frac{\sigma - 1}{\sigma}} \right\}^{\frac{\sigma - 1}{\sigma}} \beta_i^{\frac{1 - \sigma}{\sigma} - 1} c_{ij}^{\frac{\sigma - 1}{\sigma} - 1} = \mu p_{ij}.
\end{equation*}

Repeating for consumption from \(k\) , \(c_{kj}\) , and then taking
ratio of FOCs \begin{equation*}
\frac{\beta_i}{\beta_k}^{\frac{1 - \sigma}{\sigma} - 1} \frac{c_{ij}}{c_{kj}}^{\frac{\sigma - 1}{\sigma} - 1} = \frac{p_{ij}}{p_{kj}}.
\end{equation*}

Solving for \(c_kj\) in terms of \(c_ij\), for any country \(k\), and
then replacing the solutions for all countries in the budget constraint
and solving for \(c_{ij}\), the solution to the consumer's problem is
\begin{align*}
c_{ij} &= p_{ij}^{-\sigma} \left( \frac{\beta_i}{P_j} \right)^{1 - \sigma} E_j \\
P_j &= \left\{\sum_{i \in I} (\beta_i p_{ij})^{1 - \sigma} \right\}^{\frac{1}{1 - \sigma}}.
\end{align*}

Multiplying by \(p_{ij}\) to get nominal demand \(X_{ij}\) and defining
\(p_{ij} = p_i t_{ij}\) \begin{align*}
X_{ij} &= \left( \frac{\beta_i p_i t_{ij}}{P_j} \right)^{1 - \sigma} E_j \\
P_j &= \left\{\sum_{i \in I} (\beta_i p_{i} t_{ij})^{1 - \sigma} \right\}^{\frac{1}{1 - \sigma}}.
\end{align*}

Imposing market clearance condition \begin{equation*}
Y_i = \sum_{i \in I} X_{ij},\: \forall i \implies Y_i = \sum_{i \in I} \left( \frac{\beta_i p_i t_{ij}}{P_j} \right)^{1 - \sigma} E_j,\: \forall i
\end{equation*}

Dividing both sides by world income \(Y\) and defining \(\Pi\) as
follows \begin{align*}
\frac{Y_i}{Y} &= \sum_{i \in I} \left( \frac{\beta_i p_i t_{ij}}{P_j} \right)^{1 - \sigma} \frac{E_j}{Y},\: \forall i \\
\Pi^{1 - \sigma} &= \sum_{j \in J} \left(\frac{t_{ij}}{P_j}\right)^{1 - \sigma} \frac{E_j}{Y} \\
\implies \frac{Y_i}{Y} &= (\beta_i p_i \Pi_i)^{1 - \sigma},\: \forall i \\
\implies (\beta_i p_i)^{1 - \sigma} &= \frac{Y_i}{Y \Pi_i^{1 - \sigma}},\: \forall i
\end{align*}

This final term can be replaced in the expressions for \(P_j\) and
\(X_{ij}\) to obtain the structural gravity system which can be
estimated with GLMs even under market-clearing conditions \begin{align*}
X_{ij} &= \frac{E_j Y_i}{Y} \left( \frac{t_{ij}}{P_j \Pi_i} \right)^{1 - \sigma} \\
P_j^{1 - \sigma} &= \sum_{i \in I} \frac{Y_i}{Y} \left( \frac{t_{ij}}{\Pi_i} \right)^{1 - \sigma} \\
\Pi_i^{1 - \sigma} &= \sum_{j \in I} \frac{E_j}{Y} \left( \frac{t_{ij}}{P_j} \right)^{1 - \sigma} \\
E_j &= Y_j = Q_j p_j \\
Y_j &= \sum_{i \in I} X_{ij}.
\end{align*}

In this model, \(X_{ij}\) is the structural gravity equation, \(P_j\) is
the inward multilateral resistance, \(\Pi_i\) is the outward
multilateral resistance, \(E_j\) is the expenditure/income/GDP and
\(Y_j\) defines the market clearance condition.

Most of newer articles build on this model. For example, Anderson and
Van Wincoop (2003), another cornerstone article in gravity literature,
builds up from this model.

For the purpose of estimation, the structural gravity system can be
expressed as \begin{align*}
X_{ij} &= \frac{E_j Y_i}{Y} \left( \frac{t_{ij}}{P_j \Pi_i} \right)^{1 - \sigma} \label{eq:1}\\
P_j^{1 - \sigma} &= \sum_{i \in I} \frac{Y_i}{Y} \left( \frac{t_{ij}}{\Pi_i} \right)^{1 - \sigma} \\
\Pi_i^{1 - \sigma} &= \sum_{j \in I} \frac{E_j}{Y} \left( \frac{t_{ij}}{P_j} \right)^{1 - \sigma} \\
E_j &= Y_j = Q_j p_j \\
p_j &= \frac{(Y_j / Y)^{\frac{1}{1 - \sigma}}}{\beta_j \Pi_j}.
\end{align*}

\hypertarget{general-equilibrium-ppml-model}{%
\subsubsection{General equilibrium PPML
model}\label{general-equilibrium-ppml-model}}

From the core model, it is possible to estimate a General Equilibrium
PPML (GEPPML) model (Yotov et al. 2016). There are four sequential steps
that define an `auction' or `russian doll' process.

\textbf{Step 1: Solve the Baseline Gravity Model}

Step 1A is to obtain estimates of trade costs and trade elasticities.
This is done by using PPML, panel data with intervals, exporter-time
fixed effects, importer-time fixed effects, and pair fixed effects to
estimate \begin{equation*}
X_{ij,t} = \exp[T_{ij,t}\beta + \pi_{i,t} + \xi_{j,t}] \times \varepsilon_{ij,t}.
\end{equation*}

Accounting for perfect collinearity, imposing a normalization for the
MRs, and estimating structural gravity without a constant implies:
\(\pi_{i,t} = Y_{i,t} / (\Pi^{1-\sigma}_{i,t} Y_t)\) and
\(\xi_{0,t} = E_{0,t}\), where \(\xi_{0,t} = E_{0,t}\) is the dropped
fixed effect.

For this step it is crucial to select a country with good data, since it
shall be the reference factor for the model. It is also important to
select a country that is `remote' from the counterfactual. The
estimation can be done with any method from the literature, such as the
tetrads method already implemented in R (Woelwer et al. 2018).

Step 1B is to construct baseline indexes. This is done by using the
estimates of the gravity fixed effects together with data on output and
expenditure to construct the multilateral resistances \begin{align*}
\Pi^{1-\sigma}_{1,t} &= \frac{Y_{i,t}}{\exp(\hat{\pi}_{i,t})} \times \frac{E_{0,t}}{Y_t} \\
P^{1-\sigma}_{j,t} &= \frac{E_{j,t}}{\exp(\hat{\xi}_{j,t})} \times \frac{1}{E_{0,t}}.
\end{align*}

Where, by construction, \(Y_{i,t} = \sum_j X_{ij,t}\) and
\(E_{j,t} = \sum_i X_{ij,t}\). Any other baseline GE index can be
constructed, being it real consumption, terms of trade or others.

A starting point, based on partial equilibrium results, is to fit the
model \begin{align*}
X_{ij,t} =& \:\exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 INTL_{i,j}\right] \times \varepsilon_{ij,t}.
\end{align*}

The code for this model is straightforward.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_baseline\_app1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
\NormalTok{  trade }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ log\_dist }\SpecialCharTok{+}\NormalTok{ cntg }\SpecialCharTok{+}\NormalTok{ intl }\SpecialCharTok{+}\NormalTok{ exporter }\SpecialCharTok{+}\NormalTok{ importer,}
  \AttributeTok{family =} \FunctionTok{quasipoisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
  \AttributeTok{data =}\NormalTok{ ch2\_application1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

One challenge inR is to extract the estimated fixed effects coefficients
in order to construct the variables for export and import fixed effects.
This was done by implementing the \texttt{tp\_fixed\_effects()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ch2\_application1 }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(}
    \FunctionTok{tp\_fixed\_effects}\NormalTok{(fit\_baseline\_app1),}
    \FunctionTok{c}\NormalTok{(}\StringTok{"exporter"}\NormalTok{, }\StringTok{"importer"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

After performing the steps to obtain both OMRs and IMRS, to complete
this stage of the estimation, I had to create a column with the
estimated international trade for given output and expenditures. This
starts by adding a column, \texttt{tradehat\_bln}, with the regression
output, and then grouping by exporter and summarising to obtain the
required column \texttt{xi\_bln}.

\textbf{Step 2: Define a counterfactual experiment}

Consists in defining the policy experiment by changing the definition of
the policy variables in vector \$T\_\{ij,t\}. Alternatively, one may
change the values for the trade cost elasticities, the \(\beta\)'s.

On the one hand, one alternative is to eliminate the border variable and
then generate the logged trade costs used in the constraint.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ch2\_application1 }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{tij\_cfl =} \FunctionTok{exp}\NormalTok{(fit\_baseline\_app1}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"log\_dist"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ log\_dist }\SpecialCharTok{+}
\NormalTok{                  fit\_baseline\_app1}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"cntg"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ cntg),}
    \AttributeTok{log\_tij\_cfl =} \FunctionTok{log}\NormalTok{(tij\_cfl)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

On the other, a second alternative is to define a new counterfactual
border variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ch2\_application1 }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{intl\_cfl =} \DecValTok{0}\NormalTok{,}
    \AttributeTok{tij\_bln =} \FunctionTok{exp}\NormalTok{(fit\_baseline\_app1}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"log\_dist"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ log\_dist }\SpecialCharTok{+}
\NormalTok{                  fit\_baseline\_app1}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"cntg"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ cntg }\SpecialCharTok{+}
\NormalTok{                  fit\_baseline\_app1}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"intl"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ intl\_cfl),}
    \AttributeTok{log\_tij\_cfl =} \FunctionTok{log}\NormalTok{(tij\_cfl)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\textbf{Step 3: Solve the counterfactual model}

Step 3A is to obtain conditional GE effects \begin{equation*}
X_{ij,t} = \exp[T_{ij,t}^{CFL}\bar{\beta} + \pi_{i,t}^{CFT} + \xi_{j,t}^{CFT}] \times \varepsilon_{ij,t}^{CFT}.
\end{equation*}

Where, the bar symbol is used to capture the fact that certain
coefficients are constrained to be equal to their estimated counterparts
from the equation in step 1 or to their new values as defined in step 2.

This step is completed by repeating step 1A with the new gravity fixed
effects estimates and the original data on output and expenditure to
construct the conditional GE estimates of the MRs and construct any
other GE indexes of interest.

Step 3B is to estimate the full endowment gravity model. This step
delivers estimates of the fixed effects that correspond to the full
endowment scenario. This step is performed in a loop with four stages:
(i) Allow for changes in factory-gate prices; (ii) Allow for endogenous
income, expenditure, and trade; (iii) Estimate the full endowment model;
(iv) Construct full endowment GE Effects.

In practice, this steps is a recursion by fitting a model similar to the
model from step 1, the constrained gravity model, where \(\pi_{j,t}\)
and \(\chi_{j,t}\) are altered \begin{align*}
X_{ij,t} =& \:\exp\left[\pi_{i,t}^{CFL} + \chi_{i,t}^{CFL} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 INTL_{i,j}\right] \times \varepsilon_{ij,t}.
\end{align*}

A large advantage from R is to have the \texttt{offset()} function,
which makes this estimation of a counterfactual model very
straightforward.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_counterfactual\_app1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
\NormalTok{  trade }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ exporter }\SpecialCharTok{+}\NormalTok{ importer }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(log\_tij\_cfl),}
  \AttributeTok{family =} \FunctionTok{quasipoisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
  \AttributeTok{data =}\NormalTok{ ch2\_application1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From this, it's direct to compute the conditional general equilibrium
effects of trade, as this is a repetition of step 1. The problem is to
construct the loop with an iterative procedure to converge to full
endowment general equilibrium effects, this is the `auction' process
implicit in GE.

Taking the elasticities of substitution from the literature, and
building the price changes in a counterfactual scenario, the problem
reduces to run the mentioned loop of \(N\) steps, which cannot be
divided into smaller pieces because the step \(N\) depends on the step
\(N-1\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set parameters}
\NormalTok{max\_dif }\OtherTok{\textless{}{-}} \DecValTok{1}\NormalTok{; sd\_dif }\OtherTok{\textless{}{-}} \DecValTok{1}\NormalTok{; change\_price\_i\_old }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{i }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{while}\NormalTok{(sd\_dif }\SpecialCharTok{\textgreater{}} \FloatTok{1e{-}5} \SpecialCharTok{|}\NormalTok{ max\_dif }\SpecialCharTok{\textgreater{}} \FloatTok{1e{-}5}\NormalTok{) \{}
\NormalTok{  ch2\_application1 }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{trade\_1 =}\NormalTok{ tradehat\_0 }\SpecialCharTok{*}\NormalTok{ change\_p\_i\_0 }\SpecialCharTok{*}\NormalTok{ change\_p\_j\_0 }\SpecialCharTok{/} 
\NormalTok{             (change\_omr\_full\_0 }\SpecialCharTok{*}\NormalTok{ change\_imr\_full\_0))}

  \CommentTok{\# repeat the counterfactual model +}
  \CommentTok{\# compute the conditional general equilibrium effects of trade}
  \CommentTok{\# compute the change in prices for exporters and importers +}
  \CommentTok{\# compute both outward and inward multilateral resistance +}
  \CommentTok{\# compute the conditional general equilibrium effects of trade}
\NormalTok{  ...}
  
  \CommentTok{\# update the differences}
\NormalTok{  max\_dif }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(}\FunctionTok{max}\NormalTok{(ch2\_application1}\SpecialCharTok{$}\NormalTok{change\_p\_i\_0 }\SpecialCharTok{{-}}\NormalTok{ change\_price\_i\_old))}
\NormalTok{  sd\_dif }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(ch2\_application1}\SpecialCharTok{$}\NormalTok{change\_p\_i\_0 }\SpecialCharTok{{-}}\NormalTok{ change\_price\_i\_old)}
\NormalTok{  change\_price\_i\_old }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1}\SpecialCharTok{$}\NormalTok{change\_p\_i\_0}

  \CommentTok{\# compute changes in outward and inward multilateral resistance}
\NormalTok{  ch2\_application1 }\OtherTok{\textless{}{-}}\NormalTok{ ch2\_application1 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{change\_omr\_full\_1 =}\NormalTok{ omr\_cfl\_1 }\SpecialCharTok{/}\NormalTok{ omr\_cfl\_0,}
      \AttributeTok{omr\_cfl\_0 =}\NormalTok{ omr\_cfl\_1,}
\NormalTok{      ...}
\NormalTok{    )}

\NormalTok{  i }\OtherTok{\textless{}{-}}\NormalTok{ i }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The rest of the step consists in computing the full endowment general
equilibrium of trade. This short, but exhaustive summary, should have
made clear that taking GLM's output as inputs for more GLM models is
computationally costly

\textbf{Step 4: Collect, construct and report indexes of interest}

This step is direct. It consists in computing the change in full
endowment general equilibrium factory-gate price on export side
(\texttt{change\_price\_full}), the change in conditional and full
general equilibrium outward multilateral resistances
(\texttt{change\_omr\_*}), and the change in conditional and full
general equilibrium international trade (\texttt{change\_xi\_*}).

In particular, the first GE model from Yotov et al. (2016) results,
converges after 28 iterations, resulting in full replication of the
book's figures and models output.

\includegraphics{tesis-pacha_files/figure-latex/ge_plot-1.pdf}

\includegraphics{tesis-pacha_files/figure-latex/ge_plot2-1.pdf}

\hypertarget{findings}{%
\subsection{Findings}\label{findings}}

It was possible to fully replicate each model, figure and summary from
Yotov et al. (2016). The solely differences were model intercepts
because of how Stata and R differ at some steps of GLMs fitting, but the
slopes, standard errors and every model assessment metric could be fully
replicated.

The replication work was somewhat straightforward because of the
availability if: (i) code and datasets, both documented; (ii)
comprehensive explanations of the theory, which allowed to provide a
meaningful translation from Stata to R, even when the translation was
not 1:1 because of difference on how both languages deal with
categorical variables; (iii) direct communication with the principal
author, who replied all of my questions about the data cleaning steps.

\chapter{Efficient Fitting of Linear Models}

\hypertarget{bottlenecks-in-glm-estimation}{%
\section{Bottlenecks in GLM
estimation}\label{bottlenecks-in-glm-estimation}}

Besides the classic estimation problem of linear dependence when using
fixed effects, which leads to \texttt{NA} estimated country-time
specific coefficients, the estimation of relevant effects such as RTAs
over time constitutes itself a bottle neck which is not solved
necessarily by scaling hardware.

Certainly, more powerful processors allow a faster estimation for some
statistical models, but fitting times are very consistent on scaled
hardware, so a solution for competitive fitting times is related to a
different fitting algorithm.

Reproducing the traditional gravity estimates PPML model from Chapter 1
in Yotov et al. (2016) was already discussed in the previous chapter of
this thesis, the model consists in a country-time fixed effects GLM with
Quasi-Poisson link \begin{align*}
X_{ij,t} =& \:\exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} +\right.\\
\text{ }& \:\left.\beta_3 LANG_{i,j} + \beta_4 CLNY_{i,j}\right] \times \varepsilon_{ij,t}
\end{align*}

To compare GLM vs the proposed Efficient-GLM (EGLM) function, I
conducted a test with 500 repetitions on different dedicated CPUs
DigitalOcean droplets (cloud virtual machines).

\includegraphics{tesis-pacha_files/figure-latex/unnamed-chunk-4-1.pdf}

The results were very consistent across different hardware not included
in the chart, such as 2 CPUs and testing with SSD or NVMe disks.
Surprisingly enough, more CPUs don't reduce the median fitting time.
Therefore, the median time of these results can be compared against
local Stata 13 median times on the same statistical model.

\includegraphics{tesis-pacha_files/figure-latex/unnamed-chunk-5-1.pdf}

\chapter*{REFERENCES}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-malexander19}{}%
Alexander, Monica. 2019. \emph{Reproducibility in Demographic Research}.
\url{https://www.monicaalexander.com/posts/2019-10-20-reproducibility/}.

\leavevmode\hypertarget{ref-anderson1979theoretical}{}%
Anderson, James E. 1979. {``A Theoretical Foundation for the Gravity
Equation.''} \emph{The American Economic Review} 69 (1): 106--16.

\leavevmode\hypertarget{ref-anderson2003gravity}{}%
Anderson, James E, and Eric Van Wincoop. 2003. {``Gravity with Gravitas:
A Solution to the Border Puzzle.''} \emph{American Economic Review} 93
(1): 170--92.

\leavevmode\hypertarget{ref-baier2009estimating}{}%
Baier, Scott L, and Jeffrey H Bergstrand. 2009. {``Estimating the
Effects of Free Trade Agreements on International Trade Flows Using
Matching Econometrics.''} \emph{Journal of International Economics} 77
(1): 63--76.

\leavevmode\hypertarget{ref-CEPII201125}{}%
Mayer, Thierry, and Soledad Zignago. 2011. {``Notes on CEPII's Distances
Measures: The GeoDist Database.''} Working Papers 2011-25. CEPII.
\url{http://www.cepii.fr/CEPII/en/publications/wp/abstract.asp?NoDoc=3877}.

\leavevmode\hypertarget{ref-shoven1984applied}{}%
Shoven, John B, and John Whalley. 1984. {``Applied General-Equilibrium
Models of Taxation and International Trade: An Introduction and
Survey.''} \emph{Journal of Economic Literature} 22 (3): 1007--51.

\leavevmode\hypertarget{ref-silva2006log}{}%
Silva, JMC Santos, and Silvana Tenreyro. 2006. {``The Log of Gravity.''}
\emph{The Review of Economics and Statistics} 88 (4): 641--58.

\leavevmode\hypertarget{ref-cepiigeodist}{}%
Vargas, Mauricio. 2020. \emph{Cepiigeodist: CEPII's GeoDist Datasets}.
\url{https://CRAN.R-project.org/package=cepiigeodist}.

\leavevmode\hypertarget{ref-solutionsagtpa}{}%
---------. 2021a. \emph{Solutions Manual for an Advanced Guide to Trade
Policy Analysis in r}. 2nd ed. UN ESCAP.
\url{https://r.tiid.org/R_structural_gravity/}.

\leavevmode\hypertarget{ref-tradepolicy}{}%
---------. 2021b. \emph{Tradepolicy: Replication of 'an Advanced Guide
to Trade Policy Analysis'}.
\url{https://r.tiid.org/R_structural_gravity/}.

\leavevmode\hypertarget{ref-wei1996intra}{}%
Wei, Shang-Jin. 1996. {``Intra-National Versus International Trade: How
Stubborn Are Nations in Global Integration?''} National Bureau of
Economic Research.

\leavevmode\hypertarget{ref-tidyverse}{}%
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy
D'Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.
{``Welcome to the {tidyverse}.''} \emph{Journal of Open Source Software}
4 (43): 1686. \url{https://doi.org/10.21105/joss.01686}.

\leavevmode\hypertarget{ref-wickham2016r}{}%
Wickham, Hadley, and Garrett Grolemund. 2016. \emph{R for Data Science:
Import, Tidy, Transform, Visualize, and Model Data}. "OŔeilly Media,
Inc.".

\leavevmode\hypertarget{ref-Woelver2018}{}%
Woelwer, Anna Lenna, Jan Pablo Burgard, Joshua Kunst, and Mauricio
Vargas. 2018. {``Gravity: Estimation Methods for Gravity Models in r.''}
\emph{Journal of Open Source Software} 31 (3): 1038.
\url{https://doi.org/10.21105/joss.01038}.

\leavevmode\hypertarget{ref-yotov2016advanced}{}%
Yotov, Yoto V, Roberta Piermartini, José-Antonio Monteiro, and Mario
Larch. 2016. \emph{An Advanced Guide to Trade Policy Analysis: The
Structural Gravity Model}. World Trade Organization Geneva.

\end{CSLReferences}

\end{document}
